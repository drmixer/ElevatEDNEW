# Phase 6 – Internal Dashboards & Ops

Outcome: keep coverage and quality high for home users through repeatable runbooks and visibility on AI behaviour. Acceptance: weekly/bi-weekly runbooks executed, top coverage gaps fixed first, AI spot-checks on high-traffic lessons logged.

## Data surfaces
- Coverage views: `coverage_dashboard_cells` and `coverage_dashboard_rollup` (Supabase) drive the gap reports; baselines are public lesson > 0, practice ≥20 aligned items, ≥1 unit/baseline assessment, ≥1 external/linkable resource. Activity/project counts also surface for enrichment.
- CLI reports: `npm run audit:coverage-rollup` (grade/subject summary), `npm run audit:coverage-gaps` (prioritized missing baselines), `npm run audit:completeness` (missing lessons/assets/assessments/standards), `npm run audit:practice` (practice distribution).
- Usage signals: `practice_sessions` + `practice_events` joined to `lessons -> modules` show where families spend time; `student_daily_activity` backs weekly active minutes and streaks. Use these to focus fixes on high-traffic modules first. CLI: `npm run audit:traffic-report -- --days 14 --limit 25 --out test-results/ops/high_traffic_modules.txt`.
- AI monitoring: Sentry `[ai] tutor request/success/fallback/rate limit/plan gated` events plus any `api_timing` slowness. Spot-checks should capture over/under-scaffolding in `docs/ai-spot-check-log.md`.
- Prereqs: export `SUPABASE_URL` and `SUPABASE_SERVICE_ROLE_KEY`; keep usage queries pointed at prod/staging depending on the ops window.

## Cadence & runbooks

### Weekly (Mon) coverage loop
- Run CLI audits with prod/staging secrets:  
  - `npm run audit:coverage-rollup`  
  - `npm run audit:coverage-gaps`  
  - `npm run audit:completeness`  
  - `npm run audit:practice` (optional depth).  
  - `npm run audit:traffic-report -- --days 14 --limit 25 --out test-results/ops/high_traffic_modules.txt` (usage prioritization).  
  Save stdout to `test-results/ops/coverage-<YYYYMMDD>.txt` for traceability.
- Triage “no public lesson”, “practice < target”, “no unit assessment”, “no external” rows in that order. Fix flow: add/publish a core lesson, add/align practice to ≥20 (`scripts/fill_gaps_from_dashboard.ts` can auto-fill practice/assessments/external), add a unit/baseline assessment, then attach at least one enrichment link/embed.
- Re-run `npm run audit:coverage-gaps` to confirm cleared cells; if practice was autogenerated, backfill proper question content/attribution within the week.

### Bi-weekly (Thu) usage-prioritised sweep
- Fetch high-traffic modules (last 14d) to focus fixes and AI checks. Example SQL (psql/Supabase SQL editor):
  ```sql
  with recent_sessions as (
    select ps.id, ps.started_at, ps.lesson_id
    from practice_sessions ps
    where ps.started_at >= now() - interval '14 days'
  )
  select m.slug, m.title, m.subject, m.grade_band,
         count(distinct rs.id) as sessions_14d,
         count(distinct pe.id) filter (where pe.event_type in ('hint_request','system_feedback')) as ai_events_14d
  from recent_sessions rs
  join lessons l on l.id = rs.lesson_id
  join modules m on m.id = l.module_id
  left join practice_events pe on pe.session_id = rs.id
  group by m.slug, m.title, m.subject, m.grade_band
  order by sessions_14d desc
  limit 25;
  ```
- For the top 10 rows, pull coverage status: `select module_slug, meets_explanation_baseline, meets_practice_baseline, meets_assessment_baseline, meets_external_baseline from coverage_dashboard_cells where module_slug in (<slugs>);` and clear any gaps the same day (lesson > practice > assessment > external).
- Log three AI spot-checks against the busiest modules (see below). If scaffolding is off, tighten prompts in `server/ai.ts` (`baseTutorSystemPrompt`, `gradeBandGuidance`, `subjectGuidance`, `buildLearningGuardrails`) and note the change in the log.

## AI spot-check workflow
- Scope: pick the three highest-traffic modules from the usage query (Math + ELA + rotating Science/Soc Studies when present).
- How: run the lesson in the learner view, ask for a hint, a full solution, and an off-topic request to verify guardrails. Check age-appropriateness and that the tutor withholds final answers until nudged.
- Logging: append to `docs/ai-spot-check-log.md` with date, module slug/title, subject/grade, traffic metric (sessions_14d), observed behaviour (over/under-scaffolding, safety leaks, latency), and the prompt change or follow-up ticket link.

## Triage rules (fast path)
- No lesson: author/publish a core lesson first (`visibility = 'public'`, attribution present). Use `scripts/seed_intro_lessons_missing.ts` only as a temporary scaffold.
- Practice below baseline: import aligned sets or generate via `scripts/fill_gaps_from_dashboard.ts` / `scripts/import_authored_practice.ts` with `metadata.module_slug` + `metadata.standards` populated.
- No assessment: run `npm run seed:module-assessments` for broad coverage; if still missing, let `fill_gaps_from_dashboard.ts` create a baseline quiz and then replace the autogenerated items with authored ones.
- No external/enrichment: add at least one link/embed per module (`npm run import:external-resources` mappings) or let `fill_gaps_from_dashboard.ts` drop a placeholder, then swap in a vetted resource.
- Post-fix validation: rerun coverage audits, spot-check live lesson playback, and ensure Sentry stays quiet for `[ai] tutor fallback` spikes after prompt changes.

## Automation
- GitHub Actions workflow `coverage-audits.yml` runs Mon/Thu 09:30 UTC with Supabase secrets. Artifacts: `coverage-reports/*.txt` (coverage/completeness/practice/rollup/gaps) and `ops-reports/high_traffic_modules.txt` (last 14d usage + coverage flags). Pull artifacts before triage and append AI spot-checks to the log.
